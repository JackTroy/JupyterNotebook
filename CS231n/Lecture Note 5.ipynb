{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "-  Training Neural Networks,Part 1\n",
    "\n",
    "## history breakthrough:pre-training\n",
    "\n",
    "## Activation Functions\n",
    "**key:gradient flow**\n",
    "-  sigmoid function \n",
    "    - gradient vanish, sigmoid function has 0 derivatives when x < -5 or x > 5, so input must be zero-centered\n",
    "    - output not zero-centerd\n",
    "\n",
    "- tanh : same as sigmoid but one, the output is zero centerd\n",
    "\n",
    "- ReLU\n",
    "     - converge much faster, compute efficiently,\n",
    "     - kill gradient if x < 0\n",
    "     - dead neuron, the input is alway small than 0, then no update to weights\n",
    "\n",
    "- Leaky ReLU/Parametric ReLU:f(x) = max(ax,x)\n",
    "\n",
    "- Maxout:max(w1\\*x+b1,w2\\*x+b2),double weights number\n",
    "\n",
    "- suggest: use ReLU\n",
    "\n",
    "\n",
    "## Data preprocessing\n",
    "- common in machine learning\n",
    "    - original data -> zero centered data -> normalized data(zero centered data divide its std)\n",
    "    - original data -> decorrelated data(使用PCA降维，去相关,  diagonal covariance matrix) -> whitened data(covariance matrix is the identity matrix)\n",
    "  \n",
    "- in images\n",
    "    - center only\n",
    "\n",
    "## Weight Initialization\n",
    "- random guassian numbers * 0.01 : the output would be  close to 0, gradients would therefore be small\n",
    "- random guassian numbers * 1 : the output would be -1 and 1, so gradients are basically 0\n",
    "- Xavier initialization : \n",
    "    - tanh\n",
    "    - divide 2  in relu, because relu half the output variance\n",
    "\n",
    "## Batch Normalization (for x not weights) after every layer\n",
    "- Improves gradient flow through the network\n",
    "- Allows higher learning rates\n",
    "- Reduces the strong dependence on initialization\n",
    "- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe\n",
    "- note that at test time mean & std are not computed while forward passing, they are computed & stored at train time\n",
    "\n",
    "## Babysitting the Learning Process\n",
    "1. preprocess the data\n",
    "2. choose the architecture\n",
    "3. double check the loss by turning on or off the regularization\n",
    "4. make sure the model can overfit on small dataset(like 20)\n",
    "5. binary search parameters, like learning rate,make sure not two low or high\n",
    "\n",
    "## Hyperparameter Optimization\n",
    "- in a loop optimize parameters in uniformly random log space \n",
    "- no grid search, use random search above instead\n",
    "- Track the ratio of weight updates / weight magnitudes:want this to be somewhere around 0.001 or so\n",
    "\n",
    "# Lecture 6 Training Neural Networks, Part 2\n",
    "\n",
    "- problems;if no activation functinos ,then the whole networks is only linear\n",
    "\n",
    "## Parameter updates\n",
    "- Momentum update\n",
    "- Nesterov Momentum update,\n",
    "- Nesterov Accelerated Gradient\n",
    " - there is no bad local minimun, they have basically the same loss\n",
    "- AdaGrad update\n",
    "- RMSProp update\n",
    "- Adam update\n",
    "    - combination of Momentunm & AdaGrad\n",
    "    - bias correction prevent initial zero for m&v(params)\n",
    "    \n",
    "    \n",
    "## Learning rate\n",
    "- exponential decay\n",
    "- 1/t decay\n",
    "\n",
    "## Second order optimization methods\n",
    "- Quasi-Newton methods (BGFS most popular)\n",
    "- L-BFGS (Limited memory BFGS)\n",
    "\n",
    "### Inpratice\n",
    "- adam : good choice\n",
    "- L-BFGS(when able to afford full batch)\n",
    "\n",
    "## Ensembles\n",
    "- average results of different models(see the slides)\n",
    "    - can also get a small boost from averaging multiple model checkpoints of a single model. \n",
    "    - keep track of (and use at test time) a running average parameter vector\n",
    "    \n",
    "## Regularization (dropout)\n",
    "- at training time randomly set some neurons to zero in the forward pass\n",
    "- Forces the network to have a redundant representation.\n",
    "- Dropout is training a large ensemble of models (that share parameters).\n",
    "- at test time or predicting\n",
    "    - average multiple predictions with dropout while cmputing\n",
    "    - no drop out at test time, remeber to scale  \n",
    "- More common: “Inverted dropout”, test time is unchanged\n",
    "\n",
    "## Gradient checking\n",
    "- see the notes \n",
    "\n",
    "# Lecture 7 ConvNets\n",
    "\n",
    " - layer(3 dims) ->filters(squah to layer with 1 depth) ->new layer ->\n",
    " - stride, convolve speed\n",
    " - zero-padding with P = (F-1)/2. (will preserve size spatially)\n",
    " ```\n",
    " Common settings of various sizes:\n",
    "K = (powers of 2, e.g. 32, 64, 128, 512)\n",
    "- F = 3, S = 1, P = 1\n",
    "- F = 5, S = 1, P = 2\n",
    "- F = 5, S = 2, P = ? (whatever fits)\n",
    "- F = 1, S = 1, P = 0 (change the depth)\n",
    " ```\n",
    " - pooling layer, downsampling the activation map, depth remain the same\n",
    "     - max pooling\n",
    "\n",
    "# Lecture 8 spatial localization Object detection\n",
    "\n",
    "- localization , singel object\n",
    "- Object detection , multiple obect\n",
    "\n",
    "## localization\n",
    "- Localization as Regression\n",
    "    - After conv layers or last fc layer of classification model(VGG ResNet),  attach new fully-connected “regression head” to the network to and train the head as regression model\n",
    "    -output of regression head: Class agnostic v.s. Class specific, 4 numbers (one box)  v.s.C x 4 numbers (one box per class)\n",
    "- Sliding Window: Overfeat\n",
    "    - aggregate over different windows\n",
    "    - speed up computation by transforming the final fc layer to  n\\*1\\*1 conv layer (see the slides)\n",
    "\n",
    "## detection\n",
    "- Detection as Classification, too many scales & positions\n",
    "- Histogram of Oriented Gradients ,  Deformable Parts Models (CNN)\n",
    "- Region Proposals: Selective Search, use EdgeBoxes\n",
    "- R-CNN\n",
    "    - compute boxes by Region Proposals method first,  warp to CNN input size\n",
    "    - change the final fc layer of conv nets to adjust the num of classes of your task \n",
    "    - extra features using CNN\n",
    "    - train shallow model to classify region features (is a correct region or not)\n",
    "    - boxes regression ???\n",
    "- datasets : ms-coco\n",
    "- Object Detection: Evaluation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
