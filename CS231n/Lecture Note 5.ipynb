{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "\n",
    "- history breakthrough:pre-training\n",
    "\n",
    "## Activation Functions\n",
    "**key:gradient flow**\n",
    "-  sigmoid function \n",
    "    - gradient vanish, sigmoid function has 0 derivatives when x < -5 or x > 5, so input must be zero-centered\n",
    "    - output not zero-centerd\n",
    "\n",
    "- tanh : same as sigmoid but one, the output is zero centerd\n",
    "\n",
    "- ReLU\n",
    "     - converge much faster, compute efficiently,\n",
    "     - kill gradient if x < 0\n",
    "     - dead neuron, the input is alway small than 0, then no update to weights\n",
    "\n",
    "- Leaky ReLU/Parametric ReLU:f(x) = max(ax,x)\n",
    "\n",
    "- Maxout:max(w1\\*x+b1,w2\\*x+b2),double weights number\n",
    "\n",
    "- suggest: use ReLU\n",
    "\n",
    "\n",
    "## Data preprocessing\n",
    "- common in machine learning\n",
    "    - original data -> zero centered data -> normalized data(zero centered data divide its std)\n",
    "    - original data -> decorrelated data(使用PCA降维，去相关,  diagonal covariance matrix) -> whitened data(covariance matrix is the identity matrix)\n",
    "  \n",
    "- in images\n",
    "    - center only\n",
    "\n",
    "## Weight Initialization\n",
    "- random guassian numbers * 0.01 : the output would be  close to 0, gradients would therefore be small\n",
    "- random guassian numbers * 1 : the output would be -1 and 1, so gradients are basically 0\n",
    "- Xavier initialization : \n",
    "    - tanh\n",
    "    - divide 2  in relu, because relu half the output variance\n",
    "\n",
    "## Batch Normalization (for x not weights) after every layer\n",
    "- Improves gradient flow through the network\n",
    "- Allows higher learning rates\n",
    "- Reduces the strong dependence on initialization\n",
    "- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe\n",
    "- note that at test time mean & std are not computed while forward passing, they are computed & stored at train time\n",
    "\n",
    "## Babysitting the Learning Process\n",
    "1. preprocess the data\n",
    "2. choose the architecture\n",
    "3. double check the loss by turning on or off the regularization\n",
    "4. make sure the model can overfit on small dataset(like 20)\n",
    "5. binary search parameters, like learning rate,make sure not two low or high\n",
    "\n",
    "## Hyperparameter Optimization\n",
    "- in a loop optimize parameters in uniformly random log space \n",
    "- no grid search, use random search above instead\n",
    "- Track the ratio of weight updates / weight magnitudes:want this to be somewhere around 0.001 or so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
