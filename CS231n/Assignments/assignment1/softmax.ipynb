{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.347343\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n",
    "- according to the softmax function, if the data seperate uniformly, the dot product of random weights and data would be uniform as well, so fill the value into softmax function would finally result in probability close to 1 / n (num of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.248887 analytic: 1.248886, relative error: 4.264593e-08\n",
      "numerical: 2.983749 analytic: 2.983749, relative error: 9.536425e-09\n",
      "numerical: 0.769841 analytic: 0.769840, relative error: 8.576295e-08\n",
      "numerical: -0.800188 analytic: -0.800188, relative error: 7.966140e-08\n",
      "numerical: -2.514251 analytic: -2.514251, relative error: 2.205218e-08\n",
      "numerical: -0.964312 analytic: -0.964312, relative error: 5.741103e-08\n",
      "numerical: 0.065897 analytic: 0.065897, relative error: 1.228815e-06\n",
      "numerical: 0.595640 analytic: 0.595640, relative error: 5.516918e-09\n",
      "numerical: -0.553521 analytic: -0.553521, relative error: 6.338513e-08\n",
      "numerical: -1.261127 analytic: -1.261128, relative error: 3.231002e-08\n",
      "numerical: 0.026696 analytic: 0.026696, relative error: 1.363202e-06\n",
      "numerical: -1.734715 analytic: -1.734715, relative error: 3.958682e-08\n",
      "numerical: 1.454000 analytic: 1.454000, relative error: 1.056352e-08\n",
      "numerical: -0.430625 analytic: -0.430625, relative error: 1.399128e-07\n",
      "numerical: 0.107807 analytic: 0.107807, relative error: 6.049734e-07\n",
      "numerical: -0.575574 analytic: -0.575574, relative error: 4.756884e-08\n",
      "numerical: -4.150193 analytic: -4.150193, relative error: 1.178864e-08\n",
      "numerical: -0.012259 analytic: -0.012259, relative error: 2.579871e-06\n",
      "numerical: -0.176241 analytic: -0.176241, relative error: 3.766715e-07\n",
      "numerical: 1.895300 analytic: 1.895300, relative error: 3.915087e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.347343e+00 computed in 0.150550s\n",
      "vectorized loss: 2.347343e+00 computed in 0.005190s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 300: loss 934.025060\n",
      "iteration 100 / 300: loss 11.221069\n",
      "iteration 200 / 300: loss 2.178195\n",
      "iteration 0 / 300: loss 937.021096\n",
      "iteration 100 / 300: loss 10.681328\n",
      "iteration 200 / 300: loss 2.179888\n",
      "iteration 0 / 300: loss 964.942307\n",
      "iteration 100 / 300: loss 10.178848\n",
      "iteration 200 / 300: loss 2.205420\n",
      "iteration 0 / 300: loss 973.047220\n",
      "iteration 100 / 300: loss 9.667058\n",
      "iteration 200 / 300: loss 2.216146\n",
      "iteration 0 / 300: loss 986.157937\n",
      "iteration 100 / 300: loss 9.225256\n",
      "iteration 200 / 300: loss 2.098799\n",
      "iteration 0 / 300: loss 997.466946\n",
      "iteration 100 / 300: loss 8.788537\n",
      "iteration 200 / 300: loss 2.161776\n",
      "iteration 0 / 300: loss 924.673103\n",
      "iteration 100 / 300: loss 9.529958\n",
      "iteration 200 / 300: loss 2.109111\n",
      "iteration 0 / 300: loss 942.989152\n",
      "iteration 100 / 300: loss 9.042669\n",
      "iteration 200 / 300: loss 2.173463\n",
      "iteration 0 / 300: loss 952.915891\n",
      "iteration 100 / 300: loss 8.570639\n",
      "iteration 200 / 300: loss 2.141888\n",
      "iteration 0 / 300: loss 971.205627\n",
      "iteration 100 / 300: loss 8.269537\n",
      "iteration 200 / 300: loss 2.125945\n",
      "iteration 0 / 300: loss 984.383712\n",
      "iteration 100 / 300: loss 7.770316\n",
      "iteration 200 / 300: loss 2.130451\n",
      "iteration 0 / 300: loss 1000.350075\n",
      "iteration 100 / 300: loss 7.585264\n",
      "iteration 200 / 300: loss 2.165151\n",
      "iteration 0 / 300: loss 934.478895\n",
      "iteration 100 / 300: loss 8.367797\n",
      "iteration 200 / 300: loss 2.164799\n",
      "iteration 0 / 300: loss 941.799371\n",
      "iteration 100 / 300: loss 7.837620\n",
      "iteration 200 / 300: loss 2.192100\n",
      "iteration 0 / 300: loss 966.345946\n",
      "iteration 100 / 300: loss 7.530664\n",
      "iteration 200 / 300: loss 2.086098\n",
      "iteration 0 / 300: loss 967.463833\n",
      "iteration 100 / 300: loss 7.083432\n",
      "iteration 200 / 300: loss 2.023742\n",
      "iteration 0 / 300: loss 987.781147\n",
      "iteration 100 / 300: loss 6.762529\n",
      "iteration 200 / 300: loss 2.140581\n",
      "iteration 0 / 300: loss 1000.492474\n",
      "iteration 100 / 300: loss 6.396511\n",
      "iteration 200 / 300: loss 2.188782\n",
      "iteration 0 / 300: loss 929.627393\n",
      "iteration 100 / 300: loss 7.089104\n",
      "iteration 200 / 300: loss 2.179092\n",
      "iteration 0 / 300: loss 937.846994\n",
      "iteration 100 / 300: loss 6.697651\n",
      "iteration 200 / 300: loss 2.080409\n",
      "iteration 0 / 300: loss 971.875292\n",
      "iteration 100 / 300: loss 6.553389\n",
      "iteration 200 / 300: loss 2.110915\n",
      "iteration 0 / 300: loss 973.752089\n",
      "iteration 100 / 300: loss 6.216214\n",
      "iteration 200 / 300: loss 2.149398\n",
      "iteration 0 / 300: loss 986.753285\n",
      "iteration 100 / 300: loss 5.811423\n",
      "iteration 200 / 300: loss 2.094674\n",
      "iteration 0 / 300: loss 1016.330976\n",
      "iteration 100 / 300: loss 5.702206\n",
      "iteration 200 / 300: loss 2.088425\n",
      "iteration 0 / 300: loss 927.060542\n",
      "iteration 100 / 300: loss 6.249619\n",
      "iteration 200 / 300: loss 2.106179\n",
      "iteration 0 / 300: loss 943.186533\n",
      "iteration 100 / 300: loss 5.952690\n",
      "iteration 200 / 300: loss 2.132104\n",
      "iteration 0 / 300: loss 948.496307\n",
      "iteration 100 / 300: loss 5.536379\n",
      "iteration 200 / 300: loss 2.141456\n",
      "iteration 0 / 300: loss 982.561542\n",
      "iteration 100 / 300: loss 5.410481\n",
      "iteration 200 / 300: loss 2.079518\n",
      "iteration 0 / 300: loss 981.172142\n",
      "iteration 100 / 300: loss 5.210812\n",
      "iteration 200 / 300: loss 2.127438\n",
      "iteration 0 / 300: loss 1000.878247\n",
      "iteration 100 / 300: loss 4.901892\n",
      "iteration 200 / 300: loss 2.099134\n",
      "iteration 0 / 300: loss 928.851970\n",
      "iteration 100 / 300: loss 5.506572\n",
      "iteration 200 / 300: loss 2.185889\n",
      "iteration 0 / 300: loss 941.308172\n",
      "iteration 100 / 300: loss 5.244018\n",
      "iteration 200 / 300: loss 2.116829\n",
      "iteration 0 / 300: loss 960.336464\n",
      "iteration 100 / 300: loss 4.995419\n",
      "iteration 200 / 300: loss 2.028788\n",
      "iteration 0 / 300: loss 984.169753\n",
      "iteration 100 / 300: loss 4.773330\n",
      "iteration 200 / 300: loss 2.142787\n",
      "iteration 0 / 300: loss 1001.832517\n",
      "iteration 100 / 300: loss 4.671430\n",
      "iteration 200 / 300: loss 2.077341\n",
      "iteration 0 / 300: loss 1007.805000\n",
      "iteration 100 / 300: loss 4.417739\n",
      "iteration 200 / 300: loss 2.022617\n",
      "lr 3.800000e-07 reg 3.000000e+04 train accuracy: 0.325122 val accuracy: 0.338000\n",
      "lr 3.800000e-07 reg 3.050000e+04 train accuracy: 0.315367 val accuracy: 0.323000\n",
      "lr 3.800000e-07 reg 3.100000e+04 train accuracy: 0.310878 val accuracy: 0.326000\n",
      "lr 3.800000e-07 reg 3.150000e+04 train accuracy: 0.319245 val accuracy: 0.329000\n",
      "lr 3.800000e-07 reg 3.200000e+04 train accuracy: 0.306878 val accuracy: 0.330000\n",
      "lr 3.800000e-07 reg 3.250000e+04 train accuracy: 0.311980 val accuracy: 0.331000\n",
      "lr 3.960000e-07 reg 3.000000e+04 train accuracy: 0.319184 val accuracy: 0.329000\n",
      "lr 3.960000e-07 reg 3.050000e+04 train accuracy: 0.320469 val accuracy: 0.329000\n",
      "lr 3.960000e-07 reg 3.100000e+04 train accuracy: 0.320796 val accuracy: 0.340000\n",
      "lr 3.960000e-07 reg 3.150000e+04 train accuracy: 0.313694 val accuracy: 0.323000\n",
      "lr 3.960000e-07 reg 3.200000e+04 train accuracy: 0.302857 val accuracy: 0.329000\n",
      "lr 3.960000e-07 reg 3.250000e+04 train accuracy: 0.307776 val accuracy: 0.330000\n",
      "lr 4.120000e-07 reg 3.000000e+04 train accuracy: 0.312143 val accuracy: 0.337000\n",
      "lr 4.120000e-07 reg 3.050000e+04 train accuracy: 0.322918 val accuracy: 0.319000\n",
      "lr 4.120000e-07 reg 3.100000e+04 train accuracy: 0.304673 val accuracy: 0.317000\n",
      "lr 4.120000e-07 reg 3.150000e+04 train accuracy: 0.323531 val accuracy: 0.319000\n",
      "lr 4.120000e-07 reg 3.200000e+04 train accuracy: 0.315755 val accuracy: 0.333000\n",
      "lr 4.120000e-07 reg 3.250000e+04 train accuracy: 0.313224 val accuracy: 0.339000\n",
      "lr 4.280000e-07 reg 3.000000e+04 train accuracy: 0.315816 val accuracy: 0.337000\n",
      "lr 4.280000e-07 reg 3.050000e+04 train accuracy: 0.314347 val accuracy: 0.326000\n",
      "lr 4.280000e-07 reg 3.100000e+04 train accuracy: 0.318735 val accuracy: 0.333000\n",
      "lr 4.280000e-07 reg 3.150000e+04 train accuracy: 0.304143 val accuracy: 0.318000\n",
      "lr 4.280000e-07 reg 3.200000e+04 train accuracy: 0.318429 val accuracy: 0.325000\n",
      "lr 4.280000e-07 reg 3.250000e+04 train accuracy: 0.322388 val accuracy: 0.318000\n",
      "lr 4.440000e-07 reg 3.000000e+04 train accuracy: 0.311408 val accuracy: 0.322000\n",
      "lr 4.440000e-07 reg 3.050000e+04 train accuracy: 0.310878 val accuracy: 0.321000\n",
      "lr 4.440000e-07 reg 3.100000e+04 train accuracy: 0.300184 val accuracy: 0.327000\n",
      "lr 4.440000e-07 reg 3.150000e+04 train accuracy: 0.311408 val accuracy: 0.328000\n",
      "lr 4.440000e-07 reg 3.200000e+04 train accuracy: 0.314857 val accuracy: 0.324000\n",
      "lr 4.440000e-07 reg 3.250000e+04 train accuracy: 0.321265 val accuracy: 0.334000\n",
      "lr 4.600000e-07 reg 3.000000e+04 train accuracy: 0.325122 val accuracy: 0.342000\n",
      "lr 4.600000e-07 reg 3.050000e+04 train accuracy: 0.303571 val accuracy: 0.316000\n",
      "lr 4.600000e-07 reg 3.100000e+04 train accuracy: 0.317000 val accuracy: 0.341000\n",
      "lr 4.600000e-07 reg 3.150000e+04 train accuracy: 0.307633 val accuracy: 0.319000\n",
      "lr 4.600000e-07 reg 3.200000e+04 train accuracy: 0.321265 val accuracy: 0.336000\n",
      "lr 4.600000e-07 reg 3.250000e+04 train accuracy: 0.309816 val accuracy: 0.321000\n",
      "best validation accuracy achieved during cross-validation: 0.342000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [3.8e-7, 4.6e-7]\n",
    "regularization_strengths = [3e4, 3.25e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "n = 5\n",
    "rates = [i / n * (learning_rates[1] - learning_rates[0]) + learning_rates[0] \n",
    "         for i in range(n + 1)]\n",
    "strengths = [i / n * (regularization_strengths[1] - regularization_strengths[0]) \n",
    "             + regularization_strengths[0] for i in range(n + 1)]\n",
    "for rate in rates:\n",
    "    for strength in strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=rate,reg=strength,\n",
    "                  batch_size=100, num_iters=300, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        results[(rate, strength)] = (train_acc, val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "            \n",
    "          \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
